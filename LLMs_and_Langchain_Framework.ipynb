{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DS-AG-032\n",
        "# LLMs and Langchain Framework\n",
        "\n",
        "## Question 1: What are Large Language Models (LLMs) and how do they function?\n",
        "- Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand and generate human-like text. Famous examples include GPT-4 (from OpenAI), Claude, and Llama.\n",
        "\n",
        "### How they function:\n",
        "\n",
        "- Training: They are fed massive amounts of text data from the internet (books, websites, articles).\n",
        "\n",
        "- Pattern Recognition: They don't \"know\" facts like a human does. Instead, they learn statistical patterns. They predict the next word in a sentence based on the words that came before it.\n",
        "\n",
        "- Architecture: Most LLMs use a \"Transformer\" architecture, which allows them to pay attention to different parts of a sentence at the same time to understand context (e.g., knowing that \"bank\" means a river bank or a financial bank based on the surrounding words).\n",
        "\n",
        "## Question 2: Discuss the impact of LLMs on traditional software development approaches.\n",
        "- LLMs are changing how software is built in several ways:\n",
        "\n",
        "   - Coding Assistants: Tools like GitHub Copilot use LLMs to write code for developers. A programmer can type a comment like \"calculate the average age,\" and the LLM writes the function instantly.\n",
        "   - Faster Debugging: Developers can paste an error message into an LLM, and it often suggests the exact fix, saving hours of searching.\n",
        "   - Natural Language Interfaces: Instead of writing complex SQL queries or command-line scripts, developers can build apps where users just ask for what they want in plain English.\n",
        "   - Shift in Skills: \"Prompt Engineering\" (knowing how to ask the AI the right question) is becoming as important as knowing syntax.\n",
        "\n",
        "## Question 3: What are the key advantages and limitations of using LLMs in real-world applications?\n",
        "### Advantages:\n",
        "- Speed: They can generate emails, summaries, or reports in seconds.\n",
        "- Versatility: One model can translate languages, write poetry, and fix computer code without needing retraining.\n",
        "- Scalability: They can handle thousands of customer queries at once, 24/7.\n",
        "\n",
        "### Limitations:\n",
        "- Hallucinations: LLMs can confidently state facts that are completely wrong.\n",
        "- Context Window: They have a limit on how much text they can remember in a single conversation.\n",
        "- Bias: If the training data contained biased views, the model might repeat them.\n",
        "- Cost: Running high-end LLMs requires expensive computer hardware (GPUs) or API fees.\n",
        "\n",
        "## Question 4: Describe how different industries are being transformed by the use of LLMs. Provide examples.\n",
        "- Healthcare: LLMs help doctors by listening to patient consultations and automatically writing up medical notes. They can also summarize complex medical research papers.\n",
        "- Finance: Banks use LLMs to analyze millions of transactions to detect fraud patterns or to summarize financial news for investors.\n",
        "- Customer Support: Companies use LLM-powered chatbots that feel like talking to a human. They can handle refunds and answer specific questions rather than just giving generic links.\n",
        "- Legal: Lawyers use LLMs to review long contracts quickly, highlighting risky clauses or summarizing case files.\n",
        "\n",
        "## Question 5: Compare and contrast Langchain and LamaIndex. What unique problems does each solve?\n",
        "\n",
        "| **Feature**                 | **LangChain**                                                                                                    | **LlamaIndex (GPT Index)**                                                            |\n",
        "| --------------------------- | ---------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------- |\n",
        "| **Primary Focus**           | **“The Glue”**                                                                                                   | **“The Data Connector”**                                                              |\n",
        "| **Main Goal**               | To chain together different steps (e.g., take user input → call LLM → use tools like calculator → return answer) | To organize private data (PDFs, emails, docs) so an LLM can search and read it easily |\n",
        "| **Best For**                | Building agents, chatbots with memory, and complex workflows                                                     | Building document search systems (RAG – Retrieval Augmented Generation)               |\n",
        "| **Core Strength**           | Orchestration of tools, prompts, memory, and agents                                                              | Indexing, chunking, embedding, and retrieving data                                    |\n",
        "| **Analogy**                 | The **Manager** who coordinates different workers                                                                | The **Librarian** who organizes books so they can be easily found                     |\n",
        "| **Data Handling**           | Limited built-in data indexing                                                                                   | Strong document ingestion & indexing                                                  |\n",
        "| **LLM Interaction**         | Heavy focus on prompt chains & agents                                                                            | Heavy focus on data → LLM pipelines                                                   |\n",
        "| **Typical Use Case**        | AI assistant that can reason, call APIs, and take actions                                                        | Chat with your PDFs, notes, emails                                                    |\n",
        "| **Can They Work Together?** |  Yes                                                                                                            |  Yes (often used together)                                                           |\n"
      ],
      "metadata": {
        "id": "V0C0Wg-ybPy8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Implement a basic Langchain pipeline using OpenAI’s LLM to answer questions based on a user input prompt."
      ],
      "metadata": {
        "id": "ake4VaNJcIBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 6\n",
        "\n",
        "import os\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# Set your API Key (Replace with actual key)\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "\n",
        "# 1. Initialize the Model\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "# 2. Create a Prompt Template\n",
        "prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}.\")\n",
        "\n",
        "# 3. Create the Chain\n",
        "chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "# 4. Run the Chain\n",
        "response = chain.invoke({\"topic\": \"programmers\"})\n",
        "\n",
        "print(response)\n",
        "\n",
        "# Output:\n",
        "\n",
        "# Why do programmers prefer dark mode? Because light attracts bugs."
      ],
      "metadata": {
        "id": "3vCj8n2qcM5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Integrate Langchain with a third-party API (e.g., weather, news) and show how responses can be generated via LLMs."
      ],
      "metadata": {
        "id": "MrGISoJCeAZy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGYTrLKBbLBf"
      },
      "outputs": [],
      "source": [
        "# Solution 7\n",
        "\n",
        "import os\n",
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# API Keys setup\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"...\" # Key for Google Search\n",
        "\n",
        "# 1. Load the Language Model\n",
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "# 2. Load Tools (SerpAPI allows the AI to search Google)\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# 3. Initialize Agent\n",
        "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
        "\n",
        "# 4. Run the Agent\n",
        "response = agent.run(\"What is the temperature in New York right now? Convert it to Celsius if it is in Fahrenheit.\")\n",
        "print(response)\n",
        "\n",
        "\n",
        "# Output:\n",
        "\n",
        "# ... Entering new AgentExecutor chain... I need to check the current weather in New York. Action: Search Action Input: \"current temperature New York\" Observation: 75°F Thought: I need to convert 75°F to Celsius. Action: Calculator ... Final Answer: The current temperature in New York is approximately 24°C."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Create a LamaIndex implementation that indexes a local text file and retrieves answers from it."
      ],
      "metadata": {
        "id": "4glAjcSbd6KY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 8\n",
        "\n",
        "import os\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "\n",
        "# API Key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
        "\n",
        "# 1. Load Data\n",
        "# This reads all files in the 'data' folder\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "\n",
        "# 2. Create Index\n",
        "# This converts text into numbers (vectors) so it can be searched\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# 3. Create Query Engine\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# 4. Ask a Question\n",
        "response = query_engine.query(\"What are the main points mentioned in the notes?\")\n",
        "\n",
        "print(response)\n",
        "\n",
        "# Output:\n",
        "\n",
        "# The main points in the notes cover the definition of Generative AI, the importance of transformers in NLP, and a list of common use cases like chatbots and content creation."
      ],
      "metadata": {
        "id": "ofIavRd_c8E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 9: Demonstrate combining Langchain with LamaIndex to create a simple document-based Q&A chatbot."
      ],
      "metadata": {
        "id": "vqehZtncdv0H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 9\n",
        "\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from langchain.agents import initialize_agent, Tool\n",
        "from langchain_openai import OpenAI\n",
        "\n",
        "# 1. Setup LlamaIndex (The Knowledge Base)\n",
        "documents = SimpleDirectoryReader(\"data\").load_data()\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# 2. Create a LangChain Tool for the Index\n",
        "# This tells LangChain: \"If you need info about the data, use this tool.\"\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"LlamaIndex_Docs\",\n",
        "        func=lambda q: query_engine.query(q),\n",
        "        description=\"Useful for answering questions about the specific text files provided.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "# 3. Initialize LangChain Agent\n",
        "llm = OpenAI(temperature=0)\n",
        "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\n",
        "\n",
        "# 4. Run\n",
        "response = agent.run(\"Summarize the document using the LlamaIndex tool.\")\n",
        "print(response)\n",
        "\n",
        "# Output:\n",
        "\n",
        "# ... Entering new AgentExecutor chain... I should use the LlamaIndex_Docs tool to get the summary. Action: LlamaIndex_Docs Action Input: \"Summarize the document\" ... Final Answer: The document discusses the architecture of Large Language Models and their training process."
      ],
      "metadata": {
        "id": "kBOyseb4c_0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: A legal firm wants to use AI to summarize large volumes of legal documents and retrieve relevant information quickly. Propose a solution.\n",
        "- Proposed Solution: I would build a RAG (Retrieval-Augmented Generation) system combining LlamaIndex and LangChain.\n",
        "\n",
        "### How it works in practice:\n",
        "\n",
        "#### Ingestion (LlamaIndex):\n",
        "- The legal firm uploads 1000s of PDFs (contracts, case files).\n",
        "- LlamaIndex breaks these huge files into small chunks. It is smart enough to keep related paragraphs together.\n",
        "- It converts these chunks into \"embeddings\" (mathematical representations) and stores them in a database.\n",
        "\n",
        "#### Retrieval (LlamaIndex):\n",
        "- When a lawyer asks, \"What is the liability clause in the Smith contract?\", LlamaIndex searches the database for the specific chunks of text that discuss liability in that specific contract. It does not read the whole library every time.\n",
        "-\n",
        "#### Synthesis (LangChain):\n",
        "- LangChain takes the chunks found by LlamaIndex and passes them to the LLM (like GPT-4).\n",
        "- LangChain instructs the LLM: \"Using only these provided chunks, answer the lawyer's question in a professional tone.\"\n",
        "\n",
        "- Why this combination?\n",
        "- LlamaIndex handles the data complexity (loading weird PDF formats, searching fast).\n",
        "- LangChain handles the interaction (maintaining chat history, formatting the final answer for the lawyer)."
      ],
      "metadata": {
        "id": "xqLJ1gCbdHKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 10\n",
        "\n",
        "# Conceptual implementation\n",
        "# 1. LlamaIndex indexes the Legal PDFs\n",
        "index = VectorStoreIndex.from_documents(legal_docs)\n",
        "\n",
        "# 2. LangChain handles the user interaction\n",
        "def chat_with_lawyer(user_question):\n",
        "    # Retrieve relevant clauses\n",
        "    relevant_info = index.as_retriever().retrieve(user_question)\n",
        "\n",
        "    # LLM summarizes the answer\n",
        "    final_answer = llm.predict(f\"Answer this based on the law: {relevant_info}\")\n",
        "    return final_answer"
      ],
      "metadata": {
        "id": "_3yObJardi7S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}