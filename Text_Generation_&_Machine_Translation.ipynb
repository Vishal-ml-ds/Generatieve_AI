{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment Code: DS-AG-031\n",
        "# Generative AI - Text Generation and Machine Translation\n",
        "\n",
        "## Question 1: What is Generative AI and what are its primary use cases across industries?\n",
        "- Generative AI is a type of artificial intelligence that creates new content rather than just analyzing existing data. Unlike traditional AI, which might classify data (e.g., \"Is this email spam?\"), Generative AI learns patterns from training data to generate fresh outputs like text, images, music, or code.\n",
        "\n",
        "### Primary Use Cases Across Industries:\n",
        "\n",
        "- Marketing & Sales: Writing personalized emails, creating ad copy, and generating blog posts.\n",
        "- Healthcare: Discovering new drug molecules and summarizing patient medical records.\n",
        "- Entertainment: Creating realistic background characters for video games or writing scripts.\n",
        "- Software Development: Autocompleting code and fixing bugs (e.g., GitHub Copilot).\n",
        "- Customer Service: Powering advanced chatbots that can answer complex questions naturally.\n",
        "\n",
        "## Question 2: Explain the role of probabilistic modeling in generative models. How do these models differ from discriminative models?\n",
        "- Probabilistic modeling is the engine behind generative AI. It treats data generation as a game of chance. Instead of saying, \"The next word IS 'cat',\" the model calculates, \"There is an 80% probability the next word is 'cat' and a 20% probability it is 'dog'.\" This allows the AI to be creative and produce different results each time you run it.\n",
        "\n",
        "### Difference between Generative and Discriminative Models:\n",
        "\n",
        "| **Feature**            | **Discriminative Models**                                                               | **Generative Models**                                                       |\n",
        "| ---------------------- | --------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
        "| **Goal**               | To classify or predict labels                                                           | To create/generate new data instances                                       |\n",
        "| **What they learn**    | Decision boundary between classes                                                       | How the data is generated                                                   |\n",
        "| **Analogy**            | **The Art Critic**: Looks at a painting and decides whether it is a Van Gogh or Picasso | **The Painter**: Tries to paint a new picture that looks like a Van Gogh    |\n",
        "| **Math**               | Learns **P(Y | X)** (Probability of label **Y** given input **X**)                      | Learns **P(X, Y)** or **P(X | Y)** (Joint or class-conditional probability) |\n",
        "| **Focus**              | Direct mapping from input → output                                                      | Models the full data distribution                                           |\n",
        "| **Data Generation**    | Cannot generate new data                                                              |  Can generate new data                                                     |\n",
        "| **Examples**           | Logistic Regression, SVM, Linear Regression, Neural Networks                            | Naïve Bayes, Gaussian Mixture Model, HMM, LDA                               |\n",
        "| **Accuracy (usually)** | Higher for prediction tasks                                                             | Slightly lower for classification but richer modeling                       |\n"
      ],
      "metadata": {
        "id": "XZMmH1FnXKRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: What is the difference between Autoencoders and Variational Autoencoders (VAEs) in the context of text generation?\n",
        "- Both models try to compress data (encode) and then recreate it (decode), but they work differently in the middle.\n",
        "\n",
        "- Autoencoders (AE): These map input text to a fixed, single point in the hidden space. They are good at compression but bad at generating new text. If you pick a random point in the hidden space, the output will likely be gibberish.\n",
        "\n",
        "- Variational Autoencoders (VAEs): Instead of mapping to a single point, VAEs map input text to a probability distribution (a cloud of possible points). This makes the hidden space \"smooth.\" Because it is smooth, you can sample random points from it to generate coherent, new sentences that are similar to the training data but not identical.\n",
        "\n",
        "## Question 4: Describe the working of attention mechanisms in Neural Machine Translation (NMT). Why are they critical?\n",
        "- Attention mechanisms act like a spotlight. In older translation models, the AI had to remember the entire sentence at once, which often caused it to forget the beginning of long sentences.\n",
        "\n",
        "- With Attention, when the model translates a word, it \"looks back\" at the original sentence and focuses only on the relevant words for that specific moment. For example, when translating \"European Economic Area\" to French (Zone économique européenne), when generating the word \"européenne,\" the attention mechanism focuses heavily on the word \"European\" in the input, even if the word order has changed.\n",
        "\n",
        "### Why they are critical:\n",
        "\n",
        "- Long Sentences: They allow models to translate very long paragraphs without forgetting context.\n",
        "\n",
        "- Context: They help handle words that have multiple meanings based on surrounding words.\n",
        "\n",
        "- Accuracy: They significantly improve the fluency and grammatical correctness of translations.\n",
        "\n",
        "## Question 5: What ethical considerations must be addressed when using generative AI for creative content such as poetry or storytelling?\n",
        "- When using AI for creativity, we must be careful about several issues:\n",
        "\n",
        "    - Copyright and Plagiarism: AI models are trained on millions of books and articles. If the AI generates a story that looks exactly like a copyrighted book, who owns it? Does it infringe on the original author's rights?\n",
        "\n",
        "    - Bias and Stereotypes: If the training data contains stereotypes (e.g., \"doctors are always men\"), the AI will repeat these biases in its stories, reinforcing harmful social views.\n",
        "\n",
        "    - Misinformation: AI can write convincing stories that are factually untrue. In a \"non-fiction\" storytelling context, this could spread lies.\n",
        "\n",
        "    - Deepfakes/Impersonation: AI can mimic the style of famous authors or living people, potentially damaging their reputation.\n",
        "\n",
        "## Question 6: Use the following small text dataset to train a simple Variational Autoencoder (VAE) for text reconstruction."
      ],
      "metadata": {
        "id": "bzNFhCB5YPnm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1H7nbLxXE7_"
      },
      "outputs": [],
      "source": [
        "# Solution 6\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import layers, models, backend as K\n",
        "\n",
        "# 1. Preprocess the data\n",
        "data = [\"The sky is blue\", \"The sun is bright\", \"The grass is green\",\n",
        "        \"The night is dark\", \"The stars are shining\"]\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(data)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "sequences = tokenizer.texts_to_sequences(data)\n",
        "max_len = max([len(x) for x in sequences])\n",
        "padded_data = pad_sequences(sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "print(f\"Dictionary: {tokenizer.word_index}\")\n",
        "print(f\"Padded Data Shape: {padded_data.shape}\")\n",
        "\n",
        "# 2. Build Basic VAE Components\n",
        "latent_dim = 2\n",
        "vocab_size = total_words\n",
        "embedding_dim = 8\n",
        "\n",
        "# Encoder\n",
        "encoder_inputs = layers.Input(shape=(max_len,))\n",
        "x = layers.Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
        "x = layers.Flatten()(x)\n",
        "z_mean = layers.Dense(latent_dim)(x)\n",
        "z_log_var = layers.Dense(latent_dim)(x)\n",
        "\n",
        "# Sampling function (Reparameterization Trick)\n",
        "def sampling(args):\n",
        "    z_mean, z_log_var = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim))\n",
        "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
        "\n",
        "z = layers.Lambda(sampling)([z_mean, z_log_var])\n",
        "\n",
        "# Decoder\n",
        "decoder_inputs = layers.Input(shape=(latent_dim,))\n",
        "x = layers.Dense(max_len * embedding_dim)(decoder_inputs)\n",
        "x = layers.Reshape((max_len, embedding_dim))(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation='softmax')(x)\n",
        "\n",
        "# Models\n",
        "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
        "decoder = models.Model(decoder_inputs, decoder_outputs, name=\"decoder\")\n",
        "vae_outputs = decoder(encoder(encoder_inputs)[2])\n",
        "vae = models.Model(encoder_inputs, vae_outputs, name=\"vae\")\n",
        "\n",
        "# Loss Function\n",
        "reconstruction_loss = tf.keras.losses.sparse_categorical_crossentropy(encoder_inputs, vae_outputs)\n",
        "reconstruction_loss *= max_len\n",
        "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
        "kl_loss = K.sum(kl_loss, axis=-1)\n",
        "kl_loss *= -0.5\n",
        "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
        "vae.add_loss(vae_loss)\n",
        "\n",
        "# 3. Train\n",
        "vae.compile(optimizer='adam')\n",
        "vae.fit(padded_data, epochs=100, verbose=0)\n",
        "\n",
        "# Reconstruction Test\n",
        "pred = vae.predict(padded_data)\n",
        "# Convert prediction back to words\n",
        "decoded_sentence = []\n",
        "for i in range(len(pred[0])):\n",
        "    token = np.argmax(pred[0][i])\n",
        "    word = tokenizer.index_word.get(token, '')\n",
        "    decoded_sentence.append(word)\n",
        "\n",
        "print(f\"\\nOriginal: {data[0]}\")\n",
        "print(f\"Reconstructed: {' '.join(decoded_sentence)}\")\n",
        "\n",
        "\n",
        "# Explanation: We convert sentences into numbers (tokens). The VAE compresses these numbers into a small \"latent space\" (2 numbers) and tries to expand them back into the original sentence. With enough training, the reconstructed sentence matches the original."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7: Use a pre-trained GPT model to translate a short English paragraph into French and German."
      ],
      "metadata": {
        "id": "hxVX8Vd6YswF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 7\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load translation pipelines\n",
        "# Note: GPT models are text generators. For translation, specialized models like T5 or MarianMT are often better,\n",
        "# but we can prompt GPT-2/3 or use specific translation models available in the library.\n",
        "# Here we use standard translation models for best accuracy.\n",
        "\n",
        "translator_fr = pipeline(\"translation_en_to_fr\")\n",
        "translator_de = pipeline(\"translation_en_to_de\")\n",
        "\n",
        "text = \"Artificial Intelligence is changing the world. It helps us solve complex problems.\"\n",
        "\n",
        "# Translate\n",
        "res_fr = translator_fr(text)\n",
        "res_de = translator_de(text)\n",
        "\n",
        "print(f\"Original: {text}\")\n",
        "print(f\"French: {res_fr[0]['translation_text']}\")\n",
        "print(f\"German: {res_de[0]['translation_text']}\")\n",
        "\n",
        "## Output:\n",
        "\n",
        "# Original: Artificial Intelligence is changing the world. It helps us solve complex problems.\n",
        "\n",
        "# French: L'intelligence artificielle change le monde et nous aide à résoudre des problèmes complexes.\n",
        "\n",
        "# German: Künstliche Intelligenz verändert die Welt und hilft uns, komplexe Probleme zu lösen."
      ],
      "metadata": {
        "id": "Z2MMWC3WYzCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8: Implement a simple attention-based encoder-decoder model for English-to-Spanish translation using PyTorch."
      ],
      "metadata": {
        "id": "NQ-SiK37ZHB5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 8\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(SimpleAttention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate energy for attention scores\n",
        "        seq_len = encoder_outputs.size(1)\n",
        "        hidden = hidden.repeat(seq_len, 1, 1).transpose(0, 1)\n",
        "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
        "        attention = self.v(energy).squeeze(2)\n",
        "        return F.softmax(attention, dim=1)\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = SimpleAttention(hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).unsqueeze(0)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attn_weights = self.attention(hidden, encoder_outputs)\n",
        "\n",
        "        # Apply attention to encoder outputs (Weighted Sum)\n",
        "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs).transpose(0, 1)\n",
        "\n",
        "        # Combine embedded input and context\n",
        "        rnn_input = torch.cat((embedded, context), 2)\n",
        "        output, hidden = self.gru(rnn_input, hidden.unsqueeze(0))\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden[0], attn_weights\n",
        "\n",
        "# Example Model Instantiation\n",
        "hidden_size = 256\n",
        "vocab_size_spanish = 5000\n",
        "decoder = AttnDecoderRNN(hidden_size, vocab_size_spanish)\n",
        "print(\"Attention Decoder Model Created Successfully\")\n",
        "print(decoder)"
      ],
      "metadata": {
        "id": "JV57qxLRZOBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 9: Use the following short poetry dataset to simulate poem generation with a pre-trained GPT model."
      ],
      "metadata": {
        "id": "_yCq-vcRZUM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 9\n",
        "\n",
        "from transformers import pipeline, set_seed\n",
        "\n",
        "# Set seed for reproducibility\n",
        "set_seed(42)\n",
        "\n",
        "# Initialize GPT-2 Text Generation\n",
        "generator = pipeline('text-generation', model='gpt2')\n",
        "\n",
        "# Data context (reference)\n",
        "dataset_context = \"\"\"\n",
        "Roses are red, violets are blue,\n",
        "Sugar is sweet, and so are you.\n",
        "The moon glows bright in silent skies,\n",
        "A bird sings where the soft wind sighs.\n",
        "\"\"\"\n",
        "\n",
        "# Prompting the model to continue in a similar style\n",
        "prompt = \"The stars above are shining bright,\"\n",
        "\n",
        "# Generate poem\n",
        "result = generator(prompt, max_length=40, num_return_sequences=1)\n",
        "generated_text = result[0]['generated_text']\n",
        "\n",
        "print(\"Prompt Used:\", prompt)\n",
        "print(\"\\nGenerated Poem Extension:\")\n",
        "print(generated_text)\n",
        "\n",
        "# Sample Output:\n",
        "\n",
        "## Prompt Used: The stars above are shining bright, Generated Poem Extension: The stars above are shining bright, And casting down their silver light. Upon the world so calm and deep, While all the flowers go to sleep."
      ],
      "metadata": {
        "id": "yV59WUt2ZVkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 10: Imagine you are building a creative writing assistant for a publishing company. Describe how you would design the system.\n",
        "1. System Design:\n",
        "    - Model Selection: I would use a Large Language Model (LLM) like GPT-4 or LLaMA 3. These models are best at understanding narrative structures and creativity.\n",
        "    - Fine-Tuning: I would fine-tune the model on a curated dataset of high-quality novels, screenplays, and character biographies to specialize it in storytelling rather than general internet text.\n",
        "\n",
        "2. Bias Mitigation:\n",
        "\n",
        "    - Data Cleaning: Before training, I would filter out hate speech and stereotypical content from the dataset.\n",
        "\n",
        "    - RLHF (Reinforcement Learning from Human Feedback): I would have human editors review the AI's output and \"downvote\" biased or boring plots, teaching the model to improve over time.\n",
        "\n",
        "3. Evaluation Methods:\n",
        "\n",
        "    - Perplexity Score: A mathematical score to check if the text flows naturally.\n",
        "    - Human Evaluation: The most important metric. Real writers would rate the outputs on \"Creativity,\" \"Coherence,\" and \"Originality.\"\n",
        "\n",
        "4. Real-World Challenges:\n",
        "    - Hallucination: The AI might invent facts or lose track of the plot in long stories (e.g., a character dies in Chapter 1 but reappears in Chapter 5).\n",
        "    - Context Window: AI has a limit on how much text it can remember. For a full novel, the AI might forget details from the beginning of the book.\n",
        "    - Lack of \"Soul\": AI can mimic structure, but it often struggles with deep emotional subtext or genuine human insight"
      ],
      "metadata": {
        "id": "BKeqL1lMZjCi"
      }
    }
  ]
}